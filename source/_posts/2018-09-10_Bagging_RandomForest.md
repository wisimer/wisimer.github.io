---
title: Bagging 和 随机森林（Random Forest,RF）
date: 2018-09-10
categories: ML
tags: [Bagging,RF]
---

前面已经了解到`集成学习`有两个流派，一个是 Boosting 派系，它的特点是各个弱学习器之间有依赖关系。另一种是 Bagging 流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。而`随机森林`又是对 Bagging 的一个改进算法，可以很方便的并行训练。

<!--more-->

#### 一、Bagging

##### 1. Bagging 算法原理

Bagging 原理图：

![Bagging](/src/imgs/1809/0910_bagging.png)

Bagging 的弱学习器之间的确没有 Boosting 那样的联系。它的特点在“随机采样”。那么什么是随机采样？

随机采样(bootsrap)就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，则由于随机性，T个采样集各不相同。注意到这和 `GBDT` 的子采样是不同的。GBDT 的子采样是无放回采样，而 Bagging的子采样是放回采样。

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是 $\frac{1}{m}$，不被采集到的概率为 $1-\frac{1}{m}$。如果 m 次采样都没有被采集中的概率是 $(1-\frac{1}{m})^{m}$，当 $m \to \infty,(1-\frac{1}{m})^m \to \frac{1}{e} \simeq 0.368$。也就是说，在 BaggingB的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。

对于这部分大约36.8%的没有被采样到的数据，我们常常称之为`袋外数据(Out Of Bag, 简称OOB)`。这些数据没有参与训练集模型的拟合，因此可以用来`检测模型的泛化能力`。

Bagging 对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。

Bagging 的结合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。

由于 Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于`降低模型的方差`很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏差会大一些。

##### 2. Bagging 算法流程

- 输入为样本集 $D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$。
- 弱学习器算法。弱分类器迭代次数T。
- 输出为最终的强分类器f(x)
 

(1) 对于 i=1,2...,m:

  (a) 对训练集进行第 i 次随机采样，共采集 m 次，得到包含 m 个样本的采样集 D;
 
  (b) 用采样集 D 训练第 i 个弱学习器 $G_i(x)$.

(2) 如果是分类算法预测，则 m 个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，m 个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

#### 二、随机森林

理解了 Bagging 算法，随机森林(Random Forest,以下简称RF)就好理解了。它是 Bagging 算法的进化版，也就是说，它的思想仍然是 Bagging,但是进行了独有的改进。我们现在就来看看RF算法改进了什么。

首先，RF使用了`CART 决策树`作为弱学习器。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择`一个最优的特征`来做决策树的左右子树划分，但是RF通过`随机选择`节点上的`一部分样本特征`，这个数字小于n，假设为 $n_1$，然后在这些随机选择的 $n_1$ 个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　　　

如果 $n_1=n$，则此时RF的CART决策树和普通的CART决策树没有区别。$n_1$ 越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说$n_1$ 越小，模型的方差会减小，但是偏差会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的 $n_1$ 的值。

除了上面两点，RF和普通的 Bagging 算法没有什么不同， 下面简单总结下RF的算法。

- 输入为样本集 $D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$。
- 弱分类器迭代次数T。
- 输出为最终的强分类器f(x)。


(1) 对于 i=1,2...,m:

  (a)对训练集进行第 i 次随机采样，共采集 m 次，得到包含 m 个样本的采样集 D;

  (b)用采样集 D 训练第 i 个决策树模型 $G_i(x)$ ，在训练决策树模型的节点的时候，在节点上所有的样本特征中选择`一部分样本特征`， 在这些随机选择的部分样本特征中选择一个`最优的特征`来做决策树的左右子树划分

(2) 如果是分类算法预测，则 m 个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，m 个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。


#### 三、结合策略

我们可以看到不管是 Boosting 还是 Bagging，最终弱学习器都要通过结合策略结合成强学习器，结合策略有如下几个：

##### 1. 平均法

对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。

- 最简单的平均是算术平均，也就是说最终预测是：$H(x) = \frac{1}{T}\sum\limits_{1}^{T}h_i(x)$
- 如果每个个体学习器有一个权重w，则最终预测是：$H(x) = \sum\limits_{i=1}^{T}w_ih_i(x)$，其中 $wi$ 是个体学习器 $hi$ 的权重，通常有 $w_i \geq 0 ,\;\;\; \sum\limits_{i=1}^{T}w_i = 1$

##### 2. 投票法

对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是 $\{c_1,c_2,...c_K\}$,对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是 $(h_1(x), h_2(x)...h_T(x))$。

最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。

稍微复杂的投票法是绝对多数投票法，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。

更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。

##### 3. 学习法

上面两个方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。

#### 四、随机森林的推广

由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。下面对于这些RF家族的算法中有代表性的做一个总结。

##### 1. extra trees

extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：

(1) 对于每个决策树的训练集，RF 采用的是随机采样 bootstrap 来选择采样集作为每个决策树的训练集，而 extra trees 一般不采用随机采样，即每个决策树采用`原始训练集`。

(2) 在选定了划分特征后，RF的决策树会基于基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会`随机的选择一个特征值来划分决策树`。

从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，`模型的方差相对于RF进一步减少，但是偏差相对于RF进一步增大`。在某些时候，extra trees的泛化能力比RF更好。

#### 2. Totally Random Trees Embedding

Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将`低维的数据集映射到高维`，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。

TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征x划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0,  0,0,1,0,0,  0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。

映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。

##### 3. Isolation Forest
　　　　
Isolation Forest（以下简称IForest）是一种`异常点检测`的方法。它也使用了类似于RF的方法来检测异常点。

对于在 T 个决策树的样本集，IForest 也会对训练集进行随机采样,但是采样个数不需要和RF一样，对于RF，需要采样到采样集样本个数等于训练集个数。但是IForest不需要采样这么多，一般来说，`采样个数要远远小于训练集个数`。为什么呢？因为我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。

对于每一个决策树的建立， IForest采用随机选择一个划分特征，对划分特征随机选择一个划分阈值。这点也和RF不同。

另外，IForest一般会选择一个比较小的最大决策树深度 max_depth,原因同样本采集，用少量的异常点检测一般不需要这么大规模的决策树。

对于异常点的判断，则是将测试样本点 x 拟合到 T 颗决策树。计算在每颗决策树上该样本的叶子节点的深度 $h_t(x)$。，从而可以计算出平均高度h(x)。此时我们用下面的公式计算样本点x的异常概率:

$$s(x,m) = 2^{-\frac{h(x)}{c(m)}}$$

其中，m为样本个数。c(m)的表达式为：

$$c(m) =2\ln(m-1) + \xi - 2\frac{m-1}{m}, \; \xi为欧拉常数$$

s(x,m)的取值范围是[0,1],取值越接近于1，则是异常点的概率也越大。

#### 五、小结

RF算法作为一个可以高度并行化的算法，在大数据时候大有可为。下面总结一下 RF 的优缺点。 

RF的主要优点有：

1. 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。
2. 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
3. 在训练后，可以给出各个特征对于输出的重要性
4. 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
5. 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
6. 对部分特征缺失不敏感。

RF的主要缺点有：

1. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
2. 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

- - -

参考：

[1]. [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)

- - -
THE END.