<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    
    <title>Logistic回归算法 | XP</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="一、前言Logistic回归虽然名字上是叫回归，但其实它是一种分类算法。Logistic回归也在一些文献中也称为logit回归、最大熵分类(MaxEnt)或对数线性分类器。 “回归”的意思就是要找到最佳拟合参数，其中涉及的数学原理和步骤如下：  需要一个合适的分类函数来实现分类。可以使用单位阶跃函数或者Sigmoid函数。 用 代价函数 来表示 预测值h(x) 与 实际值y 的偏差 (h−y)。要">
<meta name="keywords" content="LogisticRegression">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic回归算法">
<meta property="og:url" content="http://ai.wisim.me/2018/05/18/2018-05-18_LogisticRegression/index.html">
<meta property="og:site_name" content="XP">
<meta property="og:description" content="一、前言Logistic回归虽然名字上是叫回归，但其实它是一种分类算法。Logistic回归也在一些文献中也称为logit回归、最大熵分类(MaxEnt)或对数线性分类器。 “回归”的意思就是要找到最佳拟合参数，其中涉及的数学原理和步骤如下：  需要一个合适的分类函数来实现分类。可以使用单位阶跃函数或者Sigmoid函数。 用 代价函数 来表示 预测值h(x) 与 实际值y 的偏差 (h−y)。要">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0515_classifyfunction.png">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0515_sigmidfunction.png">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0518_logistic_data_plot.png">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0518_logistic_line.png">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0518_logistic_better_gradascent.png">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0518_logistic_features_times.png">
<meta property="og:image" content="http://ai.wisim.me/src/imgs/1805/0518_logistic_numIter_5.png">
<meta property="og:updated_time" content="2018-07-24T06:24:34.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic回归算法">
<meta name="twitter:description" content="一、前言Logistic回归虽然名字上是叫回归，但其实它是一种分类算法。Logistic回归也在一些文献中也称为logit回归、最大熵分类(MaxEnt)或对数线性分类器。 “回归”的意思就是要找到最佳拟合参数，其中涉及的数学原理和步骤如下：  需要一个合适的分类函数来实现分类。可以使用单位阶跃函数或者Sigmoid函数。 用 代价函数 来表示 预测值h(x) 与 实际值y 的偏差 (h−y)。要">
<meta name="twitter:image" content="http://ai.wisim.me/src/imgs/1805/0515_classifyfunction.png">
    

    
        <link rel="alternate" href="/" title="XP" type="application/atom+xml" />
    

    
        <link rel="icon" href="https://github.com/wisimer/wisimer.github.io/blob/master/css/images/logo.png?raw=true" />
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    
        <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?c87f7ac22d7007b78c0d4fa544da6c44";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    


</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">XP</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/.">HOME</a>
                
                    <a class="main-nav-link" href="/archives">ARCHIVE</a>
                
                    <a class="main-nav-link" href="/categories">CATEGOTY</a>
                
                    <a class="main-nav-link" href="/about">ABOUT</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="想要查找什么..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/.">HOME</a></td>
                
                    <td><a class="main-nav-link" href="/archives">ARCHIVE</a></td>
                
                    <td><a class="main-nav-link" href="/categories">CATEGOTY</a></td>
                
                    <td><a class="main-nav-link" href="/about">ABOUT</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="搜索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            <section id="main"><article id="post-2018-05-18_LogisticRegression" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
            Logistic回归算法
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2018/05/18/2018-05-18_LogisticRegression/">
            <time datetime="2018-05-17T16:00:00.000Z" itemprop="datePublished">2018-05-18</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/ML/">ML</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/LogisticRegression/">LogisticRegression</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <h4 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h4><p>Logistic回归虽然名字上是叫回归，但其实它是一种分类算法。Logistic回归也在一些文献中也称为logit回归、最大熵分类(MaxEnt)或对数线性分类器。</p>
<p>“回归”的意思就是要找到最佳拟合参数，其中涉及的数学原理和步骤如下：</p>
<ol>
<li>需要一个合适的分类函数来实现分类。可以使用单位阶跃函数或者Sigmoid函数。</li>
<li>用 <code>代价函数</code> 来表示 <code>预测值h(x)</code> 与 <code>实际值y</code> 的偏差 <code>(h−y)</code>。要使得回归最佳拟合，那么偏差要尽可能小（偏差求和或取均值）。</li>
<li>记J(w,b)表示回归系数取w时的偏差，那么求最佳回归参数w,b就转换成了求J(w,b)的最小值。可以使用梯度下降法求回归参数w,b。</li>
</ol>
<p>所以，接下来就围绕这几个步骤进行展开。</p>
<a id="more"></a>
<h4 id="二、分类函数"><a href="#二、分类函数" class="headerlink" title="二、分类函数"></a>二、分类函数</h4><p>1.假设要实现二分类，那么可以找一个函数，根据不同的特征变量，输出0和1，并且只输出0和1，这种函数在某个点直接从0跳跃到1，如图：</p>
<p><img src="/src/imgs/1805/0515_classifyfunction.png" alt="classifyfunction"></p>
<p>但是这种函数处理起来，稍微有点麻烦。</p>
<p>2.我们选择另外一个连续可导的函数，也就是Sigmoid函数,函数的公式如下：</p>
<p>$\widehat{y}=h(z)=\frac{1}{1+e^{-z}} （\widehat{y}就表示预测值也就是分类结果）$</p>
<p>这个函数的特点是，当z=0时，h(z)=0.5，而z越大，h(z)越接近1，z越小，h(z)越接近0。这个函数很像阶跃函数，当z&gt;0，就可以将数据分入1类；当z &lt; 0，就可以将数据分入0类。函数图如下：</p>
<p><img src="/src/imgs/1805/0515_sigmidfunction.png" alt="sigmoid"></p>
<p>确定了分类函数，接下来，我们将Sigmoid函数的输入记为z，那么输入特征变量为：</p>
<p>$z=w_0x_0+w_1x_1+…+w_nx_n+b=w^Tx+b$，w是特征向量，b是一个值。</p>
<p>则分类函数：$\widehat{y}=h(z)=\frac{1}{1+e^{-z}}= \frac{1}{1+e^{-(w^Tx+b)}}$</p>
<p>向量x是特征变量，是输入数据，向量w,b是回归系数是特征。之后的事情就是如何确定最佳回归系数w(w0,w1,w2,…,wn,b)。</p>
<p>采用Sigmoid函数实际上就是将原始的输入x向量和某一合适的参数w向量相乘，得出的值作为sigmoid函数的输入，最终是的输出保持在0和1这两个值中的一个，从而达到分类的要求。</p>
<h4 id="三、损失函数和代价函数"><a href="#三、损失函数和代价函数" class="headerlink" title="三、损失函数和代价函数"></a>三、损失函数和代价函数</h4><p>1.损失函数（Loss function）</p>
<p>首先要知道损失函数是应用于单独一个训练样本的。而下面要说的代价函数则是针对整个训练集。</p>
<p>为了更好的进行凸优化，我们这里直接给出一个适合Logistic函数的损失函数：</p>
<p>$L(\widehat{y},y) = -[ylog\widehat{y}+(1-y)log(1-\widehat{y})]$</p>
<p>来看一下这个损失函数的特点：</p>
<ul>
<li>当y=1时，$ylog\widehat{y}$ 这个部分的值是$log\widehat{y}，\widehat{y}表示预测分类$，$(1-y)log(1-\widehat{y})$ 这个部分的值是0。所以最终的损失函数是：$L(\widehat{y},y)=-log\widehat{y}$，而我们要求损失函数越小越好，所以也就是要求 $-(log\widehat{y})$ 越小越好，也就是要求 $\widehat{y}$ 越大越好。而 $\widehat{y}$和y一样，范围都是在0-1之间，所以 $\widehat{y}$ 越接近于1（y=1），损失函数越小。</li>
<li>当y=0时，$ylog\widehat{y}$ 这个部分的值是$0，\widehat{y}表示预测分类$，$(1-y)log(1-\widehat{y})$ 这个部分的值是 $log(1-\widehat{y})$。所以最终的损失函数是：$L(\widehat{y},y)=-(1-y)log(1-\widehat{y})$，而我们要求损失函数越小越好，所以也就是要求 $-[(1-y)log(1-\widehat{y})]$ 越小越好，也就是要求 $\widehat{y}$ 越小越好，所以 $\widehat{y}$ 越接近于0（y=0），损失函数越小。</li>
</ul>
<p>2.代价函数（Cost function）或者叫 <code>成本函数</code>。</p>
<p>根据上面的损失函数可以得到整个训练集的代价函数：</p>
<p>$J(w,b)=\frac{1}{n}\sum_{i=1}^{n}{L({\widehat{y}}^i,y^i)}=-\frac{1}{n}\sum_{i=1}^{n}{[y^ilog{\widehat{y}}^i+(1-y^i)log(1-{\widehat{y}}^i)]}$</p>
<p>由于损失函数越小，预测效果越好。所以代价函数也是越小越好。由于代价函数是一个凸函数，可以采用梯度下降法求到使得代价函数最小的时候的参数值(w,b)。</p>
<h4 id="四、梯度上升-梯度下降"><a href="#四、梯度上升-梯度下降" class="headerlink" title="四、梯度上升/梯度下降"></a>四、梯度上升/梯度下降</h4><p>要使得代价函数越小越好就是要 $\frac{1}{n}\sum_{i=1}^{n}{[y^ilog{\widehat{y}}^i+(1-y^i)log(1-{\widehat{y}}^i)]}$ 越大越好。为求最大值，这里采用梯度上升的方法。</p>
<h5 id="1-损失函数的梯度："><a href="#1-损失函数的梯度：" class="headerlink" title="1. 损失函数的梯度："></a>1. 损失函数的梯度：</h5><p>首先还是要明确，损失函数是针对单独一个训练样本的。</p>
<p>由于 $L(\widehat{y},y)=-[ylog\widehat{y}+(1-y)log(1-\widehat{y})]$ ，所以损失函数 $L(\widehat{y},y)$ 对 $\widehat{y}$ 求导可得：</p>
<script type="math/tex; mode=display">\begin {aligned} d\widehat{y}=\frac{dL(\widehat{y},y)}{d\widehat{y}}
&=-\frac{y}{\widehat{y}}+\frac{1-y}{1-\widehat{y}}\\\
\end {aligned}</script><p>而 $\widehat{y}=h(z)=\frac{1}{1+e^{-z}}$，再利用链式求导法则，损失函数 $L(\widehat{y},y)$ 对 $z$ 求导可得：</p>
<script type="math/tex; mode=display">\begin {aligned}dz=\frac{dL(\widehat{y},y)}{dz}
&=\frac{dL}{d\widehat{y}} \cdot\frac{d\widehat{y}}{dz} \\\
&=(-\frac{y}{\widehat{y}}+\frac{1-y}{1-\widehat{y}})\cdot[\widehat{y}(1-\widehat{y})]\\\
&=\widehat{y}-y\\\
\end {aligned}</script><p>amazing…</p>
<p>接着再求 $dw_j$，由于 $z=w^Tx+b$ ,利用链式法则可得：</p>
<script type="math/tex; mode=display">\begin {aligned} dw_1=\frac{\partial L}{\partial w_1}
&=\frac{dL}{dz} \cdot \frac{dz}{dw_1}\\\
&=x_1 \cdot dz\\\
&=x_1 \cdot (\widehat{y}-y)\\\
\end {aligned}</script><p>同理 $dw_2=x_2(\widehat{y}-y)…dw_n=x_n(\widehat{y}-y)$。</p>
<p>最后求db: $db=\frac{dL}{dz} \cdot \frac{dz}{db}=dz=\widehat{y}-y$</p>
<p>在训练过程中我们要我们要不断更新参数w和b以使得损失函数最小，也就是梯度更新，梯度更新的逻辑是：</p>
<ul>
<li>$w_j:=w_j-\alpha \cdot dw_j$</li>
<li>$b:=b-\alpha \cdot db$</li>
<li>$\alpha$ 在这里是学习速率</li>
</ul>
<h5 id="2-代价函数的梯度"><a href="#2-代价函数的梯度" class="headerlink" title="2. 代价函数的梯度"></a>2. 代价函数的梯度</h5><p>上面已经得出了单个训练样本的梯度更新法，接着就看在整个训练集上如何使用梯度更新法。</p>
<p>由于 $\widehat{y} = \frac{1}{1+e^{-(w^Tx+b)}}$，所以 $dw_1^{(i)},dw_2^{(i)},…,dw_n^{(i)},d_b^{(i)}$ 分别表示第i个单个样本中对参数w向量中第j个分量$w_j$的导数以及对偏置b的导数。</p>
<p>令 $J(w,b)=\frac{1}{n}\sum_{i=1}^{n}{[y^ilog{\widehat{y}}^i+(1-y^i)log(1-{\widehat{y}}^i)]}$,则在整个训练集上,J(w,b)对 $w_j$的偏导数是：</p>
<script type="math/tex; mode=display">\frac{\partial J(w,b)}{\partial w_j}=\frac{1}{n}\sum_{i=1}^{n}{\frac{\partial L(\widehat{y}^{(i)},y)}{\partial w_j}}</script><p>由于对每个$w_j$都要进行梯度更新，所以我们先得到下面的伪代码：</p>
<!-- ![0518_cost_function_gradient](/src/imgs/1805/0518_cost_function_gradient.jpeg) -->
<script type="math/tex; mode=display">
\begin {aligned}
&J=0;\\\
&d_{w_1}=0,d_{w_2}=0,...d_{w_m}=0;\\\
&d_b=0;\\\
&for \: i=1 \: to \: n://n个样本\\\
&\:\:\:\:z^{(i)}=w^Tx{(i)}+b;\\\
&\:\:\:\:\widehat{y}^{i}=sigmoid(z^{i});\\\
&\:\:\:\:J+=-[y^{i}log\widehat{y}^{(i)}+(1-\widehat{i}^{i})log(1-\widehat{y}^{i})];\\\
&\:\:\:\:dz^{(i)}=\widehat{y}^{(i)}-y^{i};\\\
&\:\:\:\:for \: j=1 \: to \: m://m个参数\\\
&\:\:\:\:\:\:\:\:dw_j+=x_jdz^{(i)};\\\
&\:\:\:\:db+=dz^{(i)};\\\
&J/=n;\\\
&d_{w_1}/=n,d_{w_2}/=n,...d_{w_m}/=n;\\\
&db/=n;\\\
\end{aligned}</script><blockquote>
<p>注意这里$dw_j$没有上标，也训练集中的每个样本进行梯度运算的时候都是使用的一个全局的参数w。</p>
</blockquote>
<p>在实际操作中，不建议使用for循环，可以进行向量化，使得运算更为迅速。</p>
<h4 id="五、实现"><a href="#五、实现" class="headerlink" title="五、实现"></a>五、实现</h4><h5 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1. 数据准备"></a>1. 数据准备</h5><p><a href="/raw/code/LogisticRegression/testSet.txt">testSet.txt</a></p>
<p>数据格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-0.017612	14.053064	0</span><br><span class="line">-1.395634	4.662541	1</span><br><span class="line">-0.752157	6.538620	0</span><br><span class="line">-1.322371	7.152853	0</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>数据没有实际意义。第一列是参数x1,第二列是参数x2，第三列是分类标签（0/1）。</p>
<p>假设Sigmoid函数的输入记为 z ，那么 $z=w_{0}+w_{1}x_{1} + w_{2}x_{2}$ ，其实$w_0$就是上面的参数b。为了方便将z表示成向量相乘的形式 $z=w^Tx$，可以添加一个值为1的变量$x_0$，于是前面的z就变形为： $z=w_{0}x_{0} +w_{1}x_{1} + w_{2}x_{2}$ 。下面代码中加载数据时手动插入一列1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">从 testSet.txt 中加载数据</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataMat = []                                                        <span class="comment">#创建数据列表</span></span><br><span class="line">    labelMat = []                                                       <span class="comment">#创建标签列表</span></span><br><span class="line">    fr = open(<span class="string">'testSet.txt'</span>)                                            <span class="comment">#打开文件   </span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():                                         <span class="comment">#逐行读取</span></span><br><span class="line">        lineArr = line.strip().split()                                  <span class="comment">#去回车，放入列表</span></span><br><span class="line">        dataMat.append([<span class="number">1.0</span>, float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])          <span class="comment">#添加数据(x,y)</span></span><br><span class="line">        labelMat.append(int(lineArr[<span class="number">2</span>]))                                <span class="comment">#添加标签(分类结果)</span></span><br><span class="line">    fr.close()                                                          <span class="comment">#关闭文件</span></span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat                                            <span class="comment">#返回</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">绘制数据点图</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataMat, labelMat = loadDataSet()                                   <span class="comment">#加载数据集</span></span><br><span class="line">    dataArr = np.array(dataMat)                                         <span class="comment">#转换成numpy的array数组</span></span><br><span class="line">    n = np.shape(dataMat)[<span class="number">0</span>]                                            <span class="comment">#数据个数</span></span><br><span class="line">    xcord1 = []; ycord1 = []                                            <span class="comment">#正样本</span></span><br><span class="line">    xcord2 = []; ycord2 = []                                            <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):                                                  <span class="comment">#根据数据集标签进行分类</span></span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i]) == <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])    <span class="comment">#1为正样本</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])    <span class="comment">#0为负样本</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)                                           <span class="comment">#添加subplot</span></span><br><span class="line">    ax.scatter(xcord1, ycord1, s = <span class="number">20</span>, c = <span class="string">'red'</span>, marker = <span class="string">'s'</span>,alpha=<span class="number">.5</span>,label=<span class="string">'1'</span>) <span class="comment">#绘制1样本</span></span><br><span class="line">    ax.scatter(xcord2, ycord2, s = <span class="number">20</span>, c = <span class="string">'green'</span>,alpha=<span class="number">.5</span>,label=<span class="string">'0'</span>)             <span class="comment">#绘制0样本</span></span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">'DataSet'</span>)                                                <span class="comment">#绘制title</span></span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>); plt.ylabel(<span class="string">'x2'</span>)                                  <span class="comment">#绘制label</span></span><br><span class="line">    plt.show()                                                          <span class="comment">#显示</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    plotDataSet()</span><br></pre></td></tr></table></figure>
<p>绘制结果：<br><img src="/src/imgs/1805/0518_logistic_data_plot.png" alt="0518_logistic_data_plot"></p>
<p>$z=w^Tx$ 这个方程未知的参数为 $w_{0},w_{1},w_{2}$ ，也就是我们需要求的回归系数(最优参数)。</p>
<h5 id="2-训练Logistic回归算法"><a href="#2-训练Logistic回归算法" class="headerlink" title="2. 训练Logistic回归算法"></a>2. 训练Logistic回归算法</h5><p>回顾一下上面得出的梯度更新公式：$w_j:=w_j-\alpha \cdot dw_j$，再对照伪代码，转化成python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-inX))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">    dataMatrix = np.mat(dataMatIn)                                       <span class="comment">#变量转换成numpy的mat</span></span><br><span class="line">    labelMat = np.mat(classLabels).transpose()                           <span class="comment">#标签转换成numpy的mat,并进行转置</span></span><br><span class="line">    m, n = np.shape(dataMatrix)                                          <span class="comment">#返回dataMatrix的大小。m为行数,n为列数。</span></span><br><span class="line">    alpha = <span class="number">0.001</span>                                                        <span class="comment">#移动步长,也就是学习速率,控制更新的幅度。</span></span><br><span class="line">    maxCycles = <span class="number">500</span>                                                      <span class="comment">#最大迭代次数</span></span><br><span class="line">    weights = np.ones((n,<span class="number">1</span>))                                             <span class="comment">#weights就是要求的特征系数w，全部初始化为1</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">        h = sigmoid(dataMatrix * weights)                                <span class="comment">#梯度上升矢量化公式</span></span><br><span class="line">        dY = labelMat - h</span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * dY          <span class="comment">#对w执行梯度更新</span></span><br><span class="line">    <span class="keyword">return</span> weights.getA()                                                <span class="comment">#将矩阵转换为数组，返回权重数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet()           </span><br><span class="line">    print(gradAscent(dataMat, labelMat))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 4.12414349]</span><br><span class="line"> [ 0.48007329]</span><br><span class="line"> [-0.6168482 ]]</span><br></pre></td></tr></table></figure>
<p>也就是$w_0=4.12414349,w_1=0.48007329,w_2=-0.6168482$</p>
<p>由于决策边界 $w^Tx=0$，可得 $w_0+w_1x_1+w_2x_2=0 =&gt; x_2=(-w_0-w_1x_1)/w_2$ 。顺便根据得出的特征值绘制一下预测函数的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(weights)</span>:</span></span><br><span class="line">    dataMat, labelMat = loadDataSet()                                   <span class="comment">#加载数据集</span></span><br><span class="line">    dataArr = np.array(dataMat)                                         <span class="comment">#转换成numpy的array数组</span></span><br><span class="line">    n = np.shape(dataMat)[<span class="number">0</span>]                                            <span class="comment">#数据个数</span></span><br><span class="line">    xcord1 = []; ycord1 = []                                            <span class="comment">#正样本</span></span><br><span class="line">    xcord2 = []; ycord2 = []                                            <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):                                                  <span class="comment">#根据数据集标签进行分类</span></span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i]) == <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])    <span class="comment">#1为正样本</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])    <span class="comment">#0为负样本</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)                                           <span class="comment">#添加subplot</span></span><br><span class="line">    ax.scatter(xcord1, ycord1, s = <span class="number">20</span>, c = <span class="string">'red'</span>, marker = <span class="string">'s'</span>,alpha=<span class="number">.5</span>)<span class="comment">#绘制正样本</span></span><br><span class="line">    ax.scatter(xcord2, ycord2, s = <span class="number">20</span>, c = <span class="string">'green'</span>,alpha=<span class="number">.5</span>)            <span class="comment">#绘制负样本</span></span><br><span class="line">    x1 = np.arange(<span class="number">-3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    x2 = (-weights[<span class="number">0</span>] - weights[<span class="number">1</span>] * x1) / weights[<span class="number">2</span>]                   <span class="comment">#w0+w1x1+w2x2=0 =&gt; x2=(-w0-w1x1)/w2</span></span><br><span class="line">    ax.plot(x1, x2)</span><br><span class="line">    plt.title(<span class="string">'BestFit'</span>)                                                <span class="comment">#绘制title</span></span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>); plt.ylabel(<span class="string">'x2'</span>)                                  <span class="comment">#绘制label</span></span><br><span class="line">    plt.show()       </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet()           </span><br><span class="line">    weights = gradAscent(dataMat, labelMat)</span><br><span class="line">    plotBestFit(weights)</span><br></pre></td></tr></table></figure>
<p>如图：</p>
<p><img src="/src/imgs/1805/0518_logistic_line.png" alt="0518_logistic_line"></p>
<h4 id="六、改进-随机梯度上升法"><a href="#六、改进-随机梯度上升法" class="headerlink" title="六、改进:随机梯度上升法"></a>六、改进:随机梯度上升法</h4><h5 id="1-随机梯度算法"><a href="#1-随机梯度算法" class="headerlink" title="1. 随机梯度算法"></a>1. 随机梯度算法</h5><p>上述算法，要进行maxCycles次循环，每次循环中矩阵会有m<em>n次乘法计算，每次更新回归系数(最优参数)的时候，使用六所有的样本数据,所以时间复杂度（开销）是maxCycles</em>m*n，当数据量较大时，时间复杂度就会很大。因此，可以是用随机梯度上升法来进行算法改进，一次只用一个样本点去更新回归系数(最优参数)，这样就可以有效减少计算量了，这种方法就叫做随机梯度上升算法。</p>
<blockquote>
<p>随机梯度上升法的思想是，每次只使用一个数据样本点来更新回归系数。这样就大大减小计算开销。</p>
</blockquote>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">改进后的随机梯度下降法</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscentBetter</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m,n = np.shape(dataMatrix)                                                  <span class="comment">#返回dataMatrix的大小。m为行数,n为列数。</span></span><br><span class="line">    weights = np.ones(n)                                                        <span class="comment">#参数初始化</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        dataIndex = list(range(m))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.01</span>                                            <span class="comment">#降低alpha的大小，每次减小1/(j+i)。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))                   <span class="comment">#随机选取样本</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[randIndex]*weights))                     <span class="comment">#选择随机选取的一个样本，计算h</span></span><br><span class="line">            error = classLabels[randIndex] - h                                  <span class="comment">#计算误差</span></span><br><span class="line">            weights = weights + alpha * error * np.array(dataMatrix[randIndex]) <span class="comment">#更新回归系数,注意这里要转换为numpy.array才能正确运行</span></span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])                                           <span class="comment">#删除已经使用的样本</span></span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(weights1,weights2)</span>:</span></span><br><span class="line">    dataMat, labelMat = loadDataSet()                                   <span class="comment">#加载数据集</span></span><br><span class="line">    dataArr = np.array(dataMat)                                         <span class="comment">#转换成numpy的array数组</span></span><br><span class="line">    n = np.shape(dataMat)[<span class="number">0</span>]                                            <span class="comment">#数据个数</span></span><br><span class="line">    xcord1 = []; ycord1 = []                                            <span class="comment">#正样本</span></span><br><span class="line">    xcord2 = []; ycord2 = []                                            <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):                                                  <span class="comment">#根据数据集标签进行分类</span></span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i]) == <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])    <span class="comment">#1为正样本</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])    <span class="comment">#0为负样本</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.title(<span class="string">'BestFit'</span>)  </span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>); plt.ylabel(<span class="string">'x2'</span>)                                  <span class="comment">#绘制label</span></span><br><span class="line"></span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)                                           <span class="comment">#添加subplot</span></span><br><span class="line">    ax.scatter(xcord1, ycord1, s = <span class="number">20</span>, c = <span class="string">'red'</span>, marker = <span class="string">'s'</span>,alpha=<span class="number">.5</span>)<span class="comment">#绘制正样本</span></span><br><span class="line">    ax.scatter(xcord2, ycord2, s = <span class="number">20</span>, c = <span class="string">'green'</span>,alpha=<span class="number">.5</span>)            <span class="comment">#绘制负样本</span></span><br><span class="line"></span><br><span class="line">    x1 = np.arange(<span class="number">-3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    x2 = (-weights1[<span class="number">0</span>] - weights1[<span class="number">1</span>] * x1) / weights1[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x1, x2,label=<span class="string">'GradAscent'</span>)</span><br><span class="line"></span><br><span class="line">    x22 = (-weights2[<span class="number">0</span>] - weights2[<span class="number">1</span>] * x1) / weights2[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x1, x22,label=<span class="string">'StocGradAscent'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet()</span><br><span class="line">    start1 = time.time()</span><br><span class="line">    weights = gradAscent(dataMat, labelMat)</span><br><span class="line">    start2 = time.time()</span><br><span class="line">    weightsBetter = stocGradAscentBetter(dataMat, labelMat)</span><br><span class="line">    plotBestFit(weights,weightsBetter)</span><br></pre></td></tr></table></figure>
<p>改进后的效果和原先的差不多：</p>
<p><img src="/src/imgs/1805/0518_logistic_better_gradascent.png" alt="logistic_better_gradascent"></p>
<ul>
<li>该算法第一个改进之处在于，alpha在每次迭代的时候都会调整 : <code>alpha = 4/(1.0+j+i)+0.01</code>，并且，虽然alpha会随着迭代次数不断减小，但永远不会减小到0，因为这里还存在一个常数项。必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。如果需要处理的问题是动态变化的，那么可以适当加大上述常数项，来确保新的值获得更大的回归系数。另一点值得注意的是，在降低alpha的函数中，alpha每次减少1/(j+i)，其中j是迭代次数，i是样本点的下标。</li>
<li>第二个改进的地方在于更新回归系数(最优参数)时，只使用一个样本点，并且选择的样本点是随机的，每次迭代不使用已经用过的样本点。这样的方法，就有效地减少了计算量，并保证了回归效果。</li>
</ul>
<h5 id="2-回归系数与迭代次数的关系"><a href="#2-回归系数与迭代次数的关系" class="headerlink" title="2. 回归系数与迭代次数的关系"></a>2. 回归系数与迭代次数的关系</h5><p>来直观地看一下回归系数是怎么样变化地：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">    weights_array = np.array([])</span><br><span class="line">    dataMatrix = np.mat(dataMatIn)                                       <span class="comment">#变量转换成numpy的mat</span></span><br><span class="line">    labelMat = np.mat(classLabels).transpose()                           <span class="comment">#标签转换成numpy的mat,并进行转置</span></span><br><span class="line">    m, n = np.shape(dataMatrix)                                          <span class="comment">#返回dataMatrix的大小。m为行数,n为列数。</span></span><br><span class="line">    alpha = <span class="number">0.001</span>                                                        <span class="comment">#移动步长,也就是学习速率,控制更新的幅度。</span></span><br><span class="line">    maxCycles = <span class="number">500</span>                                                      <span class="comment">#最大迭代次数</span></span><br><span class="line">    weights = np.ones((n,<span class="number">1</span>))                                             <span class="comment">#weights就是要求的特征系数w，全部初始化为1</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">        h = sigmoid(dataMatrix * weights)                                <span class="comment">#梯度上升矢量化公式</span></span><br><span class="line">        dY = labelMat - h</span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * dY          <span class="comment">#对w执行梯度更新</span></span><br><span class="line">        weights_array = np.append(weights_array,weights)</span><br><span class="line">    weights_array = weights_array.reshape(maxCycles,n)    </span><br><span class="line">    <span class="keyword">return</span> weights.getA(),weights_array                                         <span class="comment">#将矩阵转换为数组，返回权重数组</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscentBetter</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">5</span>)</span>:</span></span><br><span class="line">    m,n = np.shape(dataMatrix)                                                  <span class="comment">#返回dataMatrix的大小。m为行数,n为列数。</span></span><br><span class="line">    weights = np.ones(n)                                                        <span class="comment">#参数初始化</span></span><br><span class="line">    weights_array = np.array([])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        dataIndex = list(range(m))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.01</span>                                            <span class="comment">#降低alpha的大小，每次减小1/(j+i)。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))                   <span class="comment">#随机选取样本</span></span><br><span class="line">            h = sigmoid(sum(dataMatrix[randIndex]*weights))                     <span class="comment">#选择随机选取的一个样本，计算h</span></span><br><span class="line">            error = classLabels[randIndex] - h                                  <span class="comment">#计算误差</span></span><br><span class="line">            weights = weights + alpha * error * np.array(dataMatrix[randIndex]) <span class="comment">#更新回归系数,注意这里要转换为numpy.array才能正确运行</span></span><br><span class="line">            weights_array = np.append(weights_array,weights,axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])                                           <span class="comment">#删除已经使用的样本</span></span><br><span class="line">    <span class="keyword">return</span> weights,weights_array</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">绘制回归参数和迭代次数的关系</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotWeights</span><span class="params">(weights_array0,weights_array1)</span>:</span></span><br><span class="line">    <span class="comment">#将fig画布分隔成1行1列,不共享x轴和y轴,fig画布的大小为(13,8)</span></span><br><span class="line">    <span class="comment">#当nrow=3,nclos=2时,代表fig画布被分为六个区域,axs[0][0]表示第一行第一列</span></span><br><span class="line">    fig, axs = plt.subplots(nrows=<span class="number">3</span>, ncols=<span class="number">2</span>,sharex=<span class="keyword">False</span>, sharey=<span class="keyword">False</span>, figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    x0 = np.arange(<span class="number">0</span>, len(weights_array0), <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#绘制w0与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">1</span>].plot(x0,weights_array0[:,<span class="number">0</span>])</span><br><span class="line">    axs0_title_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_title(<span class="string">'GradAscent:weight and times'</span>)</span><br><span class="line">    axs0_ylabel_text = axs[<span class="number">0</span>][<span class="number">0</span>].set_ylabel(<span class="string">'w0'</span>)</span><br><span class="line">    plt.setp(axs0_title_text, size=<span class="number">20</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs0_ylabel_text, size=<span class="number">20</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment">#绘制w1与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">1</span>][<span class="number">1</span>].plot(x0,weights_array0[:,<span class="number">1</span>])</span><br><span class="line">    axs0_ylabel_text = axs[<span class="number">1</span>][<span class="number">0</span>].set_ylabel(<span class="string">'w1'</span>)</span><br><span class="line">    plt.setp(axs0_ylabel_text, size=<span class="number">20</span>,  color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment">#绘制w2与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">2</span>][<span class="number">1</span>].plot(x0,weights_array0[:,<span class="number">2</span>])</span><br><span class="line">    axs0_xlabel_text = axs[<span class="number">2</span>][<span class="number">0</span>].set_xlabel(<span class="string">'times'</span>)</span><br><span class="line">    axs0_ylabel_text = axs[<span class="number">2</span>][<span class="number">0</span>].set_ylabel(<span class="string">'w2'</span>)</span><br><span class="line">    plt.setp(axs0_xlabel_text, size=<span class="number">20</span>, color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs0_ylabel_text, size=<span class="number">20</span>, color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line">    x1 = np.arange(<span class="number">0</span>, len(weights_array1)/<span class="number">3</span>, <span class="number">1</span>) <span class="comment">#由于weights_array1是一个一行n列的数组，保存列所有的参数值，这里要处以参数的个数3</span></span><br><span class="line">    <span class="comment">#绘制w0与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">0</span>][<span class="number">0</span>].plot(x1,weights_array1[<span class="number">0</span>::<span class="number">3</span>]) <span class="comment">#[0::3]表示从位置0开始每隔3个取一位</span></span><br><span class="line">    axs1_title_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_title(<span class="string">'BetterStocGradAscent:weight and times'</span>)</span><br><span class="line">    axs1_ylabel_text = axs[<span class="number">0</span>][<span class="number">1</span>].set_ylabel(<span class="string">'w0'</span>)</span><br><span class="line">    plt.setp(axs1_title_text, size=<span class="number">20</span>,  color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs1_ylabel_text, size=<span class="number">20</span>,  color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment">#绘制w1与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">1</span>][<span class="number">0</span>].plot(x1,weights_array1[<span class="number">1</span>::<span class="number">3</span>]) <span class="comment">#[1::3]表示从位置1开始每隔3个取一位</span></span><br><span class="line">    axs1_ylabel_text = axs[<span class="number">1</span>][<span class="number">1</span>].set_ylabel(<span class="string">'w1'</span>)</span><br><span class="line">    plt.setp(axs1_ylabel_text, size=<span class="number">20</span>,  color=<span class="string">'black'</span>)</span><br><span class="line">    <span class="comment">#绘制w2与迭代次数的关系</span></span><br><span class="line">    axs[<span class="number">2</span>][<span class="number">0</span>].plot(x1,weights_array1[<span class="number">2</span>::<span class="number">3</span>]) <span class="comment">#[2::3]表示从未知2开始每隔3个取一位</span></span><br><span class="line">    axs1_xlabel_text = axs[<span class="number">2</span>][<span class="number">1</span>].set_xlabel(<span class="string">'times'</span>)</span><br><span class="line">    axs1_ylabel_text = axs[<span class="number">2</span>][<span class="number">1</span>].set_ylabel(<span class="string">'w2'</span>)</span><br><span class="line">    plt.setp(axs1_xlabel_text, size=<span class="number">20</span>,  color=<span class="string">'black'</span>)</span><br><span class="line">    plt.setp(axs1_ylabel_text, size=<span class="number">20</span>, color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet()</span><br><span class="line"></span><br><span class="line">    start1=time.time()</span><br><span class="line">    weights0,weights_array0 = gradAscent(dataMat, labelMat)</span><br><span class="line">    print(time.time()-start1)</span><br><span class="line"></span><br><span class="line">    start2=time.time()</span><br><span class="line">    weights1,weights_array1 = stocGradAscentBetter(np.array(dataMat), labelMat)</span><br><span class="line">    print(time.time()-start2)</span><br><span class="line"></span><br><span class="line">    plotWeights(weights_array0, weights_array1)</span><br><span class="line">    plotBestFit(weights0,weights1)</span><br></pre></td></tr></table></figure>
<p>注意 <code>gradAscent</code>和 <code>stocGradAscentBetter</code> 方法中分别添加了一个全局变量 <code>weights_array</code> 用来保存所有迭代出来地回归参数。</p>
<p>下面这是将 <code>stocGradAscentBetter</code>方法中的<code>numIter</code>参数设置成5的运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行时间：</span><br><span class="line">gradAscent ： 0.0238189697265625</span><br><span class="line">stocGradAscentBetter ： 0.009566068649291992</span><br></pre></td></tr></table></figure>
<p><img src="/src/imgs/1805/0518_logistic_features_times.png" alt="logistic_features_times"><br><img src="/src/imgs/1805/0518_logistic_numIter_5.png" alt="logistic_numIter_5"></p>
<p>可以看到随机梯度上升算法的迭代时间比原始的梯度上升算法少很多，但是拟合效果却差别不大。并且回归参数也更快的趋于平稳。可以试着把numIter参数增大或减小，看看具体效果。</p>
<hr>
<p>附：</p>
<p><a href="/raw/code/LogisticRegression/logistic_base.ipynb">code</a></p>
<p>参考：</p>
<ol>
<li><a href="http://scikit-learn.org/stable/modules/linear_model.html" target="_blank" rel="noopener">广义线性模型(Generalized Linear Models)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28922957" target="_blank" rel="noopener">Python3《机器学习实战》学习笔记（六）：Logistic回归基础篇之梯度上升算法</a></li>
<li><a href="https://blog.csdn.net/moxigandashu/article/details/72779856" target="_blank" rel="noopener">机器学习之Logistic回归与Python实现</a></li>
<li><a href="https://blog.csdn.net/lu597203933/article/details/38468303" target="_blank" rel="noopener">机器学习实战笔记5(logistic回归)</a></li>
<li><a href="https://github.com/Kivy-CN/dlai-notes/blob/master/1.nndl/2.3.md" target="_blank" rel="noopener">Logistic 回归损失函数</a></li>
<li><a href="https://juejin.im/post/5a4e21b46fb9a01ca267474d" target="_blank" rel="noopener">Logistic分类函数</a></li>
</ol>
<hr>
<p>THE END.</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">


    <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more">分享到：</a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间">QQ空间</a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博">新浪微博</a>
    <a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博">腾讯微博</a>
    <a href="#" class="bds_renren" data-cmd="renren" title="分享到人人网">人人网</a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信">微信</a>
</div>
<script>
window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
<style>
    .bdshare_popup_box {
        border-radius: 4px;
        border: #e1e1e1 solid 1px;
    }
    .bdshare-button-style0-16 a,
    .bdshare-button-style0-16 .bds_more {
        padding-left: 20px;
        margin: 6px 10px 6px 0;
    }
    .bdshare_dialog_list a,
    .bdshare_popup_list a,
    .bdshare_popup_bottom a {
        font-family: 'Microsoft Yahei';
    }
    .bdshare_popup_top {
        display: none;
    }
    .bdshare_popup_bottom {
        height: auto;
        padding: 5px;
    }
</style>


</div>

            
    
        <a href="http://ai.wisim.me/2018/05/18/2018-05-18_LogisticRegression/#comments" class="article-comment-link">评论</a>
    

        </footer>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2018/05/29/2018-05-29_SVM_part1/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">上一篇</strong>
            <div class="article-nav-title">
                
                    支持向量机 SVM：SVM简介
                
            </div>
        </a>
    
    
        <a href="/2018/05/14/2018-05-14_BayesPraticeEmail/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">下一篇</strong>
            <div class="article-nav-title">朴素贝叶斯分类算法实践-垃圾邮件过滤器</div>
        </a>
    
</nav>


    
</article>


    
    
        <section id="comments">
    <div id="lv-container" data-id="city" data-uid=MTAyMC8zNDc5NS8xMTMzMg==></div>
</section>
    

</section>
            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2018 Wisimer<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
        
    
    
    <!-- 来必力City版安装代码 -->
    <script type="text/javascript">
     (function(d, s) {
         var j, e = d.getElementsByTagName(s)[0];

         if (typeof LivereTower === 'function') { return; }

         j = d.createElement(s);
         j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
         j.async = true;

         e.parentNode.insertBefore(j, e);
     })(document, 'script');
    </script>
  <noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
  <!-- City版安装代码已完成 -->





    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>