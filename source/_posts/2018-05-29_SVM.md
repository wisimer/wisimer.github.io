---
title: 支持向量机 SVM
date: 2018-05-29
categories: 机器学习
tags: [SVM]
---

支持向量机(support vector machine)，一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。

SVM 原理分为软间隔最大化、拉格朗日对偶、最优化问题求解、核函数、序列最小优化 SMO 等部分。

<!--more-->

#### 一、间隔和软间隔最大化

##### 1. 分隔超平面:

![svm_intuition.png](/src/imgs/1805/0529_svm_intuition.png)

对于二分类问题来说，中间这条实线（或三维空间里的平面），就叫做分隔超平面，表达式为 $\vec w \cdot \vec x + b=0$。

##### 2. 函数间隔

SVM是通过超平面将样本分为两类。
在超平面 $w\cdot x+b=0$ 确定的情况下，$|w\cdot x+b|$ 可以相对地表示点x距离超平面的远近。对于两类分类问题，如果 $w\cdot x+b>0$ ，则x的类别被判定为1；否则判定为-1。

所以如果 $y(w\cdot x+b)>0$ ，则认为x的分类结果是正确的，否则是错误的。且 $y(w\cdot x+b)$ 的值越大，分类结果的确信度越大。反之亦然。所以样本点 $(x_{i}, y_{i})$ 与超平面(w, b)之间的函数间隔定义为:

$$\gamma_i=y_i(wx_i+b)$$

我们用 $\hat{\gamma}$ 表示函数间隔，$\hat{\gamma}=y(w \cdot x+b)$。

##### 3. 几何间隔(软间隔)

上面函数间隔的定义存在问题：即w和b同时缩小或放大M倍后，超平面并没有变化，但是函数间隔却变化了。所以，需要将w的大小固定。例如||w||=1，使得函数间隔固定，这时的间隔也就是几何间隔 。几何间隔的定义如下:

$$\gamma_{i} = y_{i} (\frac{w}{||w||}\cdot x_{i} + \frac{b}{||w||}) = y_i(\frac{w\cdot x_i+b}{||w||})$$


其中 xi 代表第 i 条数据，yi 代表第 i 条数据对应的目标变量的取值，取值有+1 和-1 两种。所以当第 i 条数据被正确分类时，y 取值和 w*x+b 取值的正负一致，几何间隔为正；当被错误分类时，y 取值和 w*x+b 取值的正负相反，几何间隔为负。如下图：

![0529_svm_yi](/src/imgs/1805/0529_svm_yi.webp)

我们用 $\tilde{\gamma}$ 表示几何间隔，$\tilde{\gamma}=y\frac{w \cdot x+b}{||w||}$。由于 $\hat{\gamma}=y(w \cdot x+b)$，所以 $\tilde{\gamma}=\frac{\hat{\gamma}}{||w||}$。

此外，函数间隔以及几何间隔其实并不是表示某种距离，真正的点到直线（或平面）的距离公式是：$\gamma = \frac{w \cdot x + b}{||w||}$。

小结一下：

1. 实际距离 ：$\gamma = \frac{w \cdot x + b}{||w||}$
2. 函数距离 ：$\hat{\gamma}=y(w \cdot x+b)$
3. 几何距离 ：$\tilde{\gamma}=y\frac{w \cdot x+b}{||w||}=\frac{\hat{\gamma}}{||w||}$

##### 4. 软间隔最大化

SVM 的核心思路是最大化支持向量到分隔超平面的间隔。后面所有的推导都是以最大化此间隔为核心思想展开。

1. 首先定义几何间隔中最小的为：$\tilde{\gamma}=min\{\tilde{\gamma_i}\}$ ，
2. 由此可以得到间隔最大化问题的目标函数 $max \{\tilde{\gamma_i}\}$，并遵循如下约束条件：$s.t.\tilde{\gamma_i} = \frac{y_i(w\cdot x_i+b)}{||w||}\geq \tilde{\gamma}$，（s.t.表示 subject to：满足...限制） 。

这里重点解释一下上面这句话的意思：

> 1.几何间隔就是点到给定超平面的距离；几何间隔中最小值 $min\{\tilde{\gamma_i}\}$ 表示所有样本点中几何间隔的最小值。

> 2.间隔最大化问题 $max \{\tilde{\gamma}\}$ 表示要找到一种情况使得最小几何间隔在所有情况中取最大值，记为 $M = arg\:max_{w,b}\{\frac{min_{n}[y_i(w^T \cdot x_i+b)]}{||w||}\}$。

##### 5. 目标函数

为了方便计算，令函数距离的最小值 $min\{\hat{\gamma}\}=min\{y(w \cdot x+b) \}= 1$，则 $M=arg \: max_{w,b}{\{\frac{1}{||w||}\}}$。很显然，`软间隔最大化` 就是要求M这个最大值，也就是要求 $\frac{1}{||w||}$的最大值，也就是要求 ||w|| 的最小值。此时几何间隔的最大化情况就是：$\tilde{\gamma} = \frac{1}{||w||}$。

于是就得出了目标函数 : $max \: {\frac{1}{||w||}},s.t.y_i(w \cdot x_i+b) \geq 1,i=1,2,...,n$。

等价于 : $min \: \frac{1}{2} ||w||^2,s.t.y_i(w \cdot x_i+b) \geq 1,i=1,2,...,n$

这是一个拉格朗日优化问题，可以通过拉格朗日乘数法得到最优超平面的权重向量W和偏置 b :

$$L(w,b,a)=\frac{1}{2}||w||^2+\sum_{i=1}^{n}a_i(y_i(w \cdot x+b)-1)$$

这样就只用一个函数就表达了要求的问题。拉格朗日乘数法也就是可以用求导来得出超平面的参数 w,b 了。有时候使用求导的方法求解依然困难，那就要用到下面的拉格朗日对偶方法求解了。

#### 二、拉格朗日对偶（Lagrange duality）

 由于这个问题的特殊结构，还可以通过拉格朗日对偶性变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是 `线性可分` 条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入 `核函数`，进而推广到 `非线性分类` 问题。

- - -

参考：

1. [Python3《机器学习实战》学习笔记（八）：支持向量机原理篇之手撕线性SVM](https://zhuanlan.zhihu.com/p/29604517)

2. [支持向量机的原理和实现](https://mp.weixin.qq.com/s/zHCgOHsBYCkPouBFTmWgpw)

3. [机器学习与数据挖掘-支持向量机(SVM)](https://wizardforcel.gitbooks.io/dm-algo-top10/content/svm-1.html)

4. [支持向量机(SVM)是什么意思？](https://www.zhihu.com/question/21094489)

5. [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)

6. [Support Vector Machines](http://cs229.stanford.edu/notes/cs229-notes3.pdf)

- - -
THE END.
