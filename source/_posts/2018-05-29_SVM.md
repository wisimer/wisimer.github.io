---
title: 支持向量机 SVM
date: 2018-05-29
categories: 机器学习
tags: [SVM]
---


支持向量:

分隔超平面:

SVM 原理分为软间隔最大化、拉格朗日对偶、最优化问题求解、核函数、序列最小优化 SMO 等部分。

#### 函数间隔

SVM是通过超平面将样本分为两类。
在超平面 $w\cdot x+b=0$ 确定的情况下，$|w\cdot x+b|$ 可以相对地表示点x距离超平面的远近。对于两类分类问题，如果 $w\cdot x+b>0$ ，则x的类别被判定为1；否则判定为-1。

所以如果 $y(w\cdot x+b)>0$ ，则认为x的分类结果是正确的，否则是错误的。且 $y(w\cdot x+b)$ 的值越大，分类结果的确信度越大。反之亦然。所以样本点 $(x_{i}, y_{i})$ 与超平面(w, b)之间的函数间隔定义为:

$\gamma_i=y_i(wx_i+b)$

#### 几何间隔(软间隔)

但是函数间隔的定义存在问题：即w和b同时缩小或放大M倍后，超平面并没有变化，但是函数间隔却变化了。所以，需要将w的大小固定，例如||w||=1，使得函数间隔固定。这时的间隔也就是几何间隔 。几何间隔的定义如下:

$\gamma_{i} = y_{i} (\frac{w}{||w||}\cdot x_{i} + \frac{b}{||w||}) = y_i(\frac{w\cdot x_i+b}{||w||})$ , 实际上，几何间隔就是点到超平面的距离。

其中 xi 代表第 i 条数据，yi 代表第 i 条数据对应的目标变量的取值，取值有+1 和-1 两种。所以当第 i 条数据被正确分类时，y 取值和 w*x+b 取值的正负一致，几何间隔为正；当被错误分类时，y 取值和 w*x+b 取值的正负相反，几何间隔为负。如下图：

![0529_svm_yi](/src/imgs/1805/0529_svm_yi.webp)

#### 软间隔最大化

SVM 的核心思路是最大化支持向量到分隔超平面的间隔。后面所有的推导都是以最大化此间隔为核心思想展开。

(1). 首先定义几何间隔中最小的为：$\gamma=min\{\gamma_i\}$ ，由此可以得到间隔最大化问题的目标函数 $max \{\gamma\}$，并遵循如下约束条件：$s.t.\gamma_i = y_i(\frac{w\cdot x_i+b}{||w||})\geq\gamma$，（s.t.表示 subject to：满足...限制） 。

> 这里重点解释一下上面这句话的意思：几何间隔就是点到给定超平面的距离；几何间隔中最小值 $min\{\gamma_i\}$ 表示所有样本点中几何间隔的最小值；间隔最大化问题 $max \{\gamma\}$ 表示要找到一种情况使得最小几何间隔在所有情况中取最大值，记为 $arg\:max_{w,b}\{\frac{1}{||w||}\:min_{n}[y_i(w^T \cdot x_i+b)]\}$。

#### 目标函数


- - -

参考：

1. [Python3《机器学习实战》学习笔记（八）：支持向量机原理篇之手撕线性SVM](https://zhuanlan.zhihu.com/p/29604517)

2. [支持向量机的原理和实现](https://mp.weixin.qq.com/s/zHCgOHsBYCkPouBFTmWgpw)

3. [机器学习与数据挖掘-支持向量机(SVM)](https://wizardforcel.gitbooks.io/dm-algo-top10/content/svm-1.html)

4. [支持向量机(SVM)是什么意思？](https://www.zhihu.com/question/21094489)

5. [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)

6. [Support Vector Machines](http://cs229.stanford.edu/notes/cs229-notes3.pdf)

- - -
THE END.
