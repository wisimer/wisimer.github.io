---
title: 朴素贝叶斯分类算法原理
date: 2018-03-27
categories: ML
tags: [NaiveBayes]
---

#### 一、朴素贝叶斯分类简介

朴素贝叶斯（Naive Bayesian）是基于贝叶斯定理和特征条件独立假设的分类方法，它通过特征计算分类的概率，选取概率大的情况进行分类，因此它是基于概率论的一种机器学习分类方法。因为分类的目标是确定的，所以也是属于监督学习。

朴素贝叶斯有如下几种：

- 离散型朴素贝叶斯: MultinomialNB
- 连续型朴素贝叶斯: GaussianNB
- 混合型朴素贝叶斯: MergedNB

<!--more-->

#### 二、原理

朴素贝叶斯基于条件概率、贝叶斯定理和独立性假设原则。

(1) 条件概率原理原理

```
基于概率论的方法告诉我们，当只有两种分类时：
如果p1(x,y)>p2(x,y)，那么分入类别1
如果p1(x,y)<p2(x,y)，那么分入类别2
```

(2) 贝叶斯定理

$p(c_i|x,y)=\frac{p(x,y|c_i)p(c_i)}{p(x,y)}$

其中，x,y表示特征变量，ci表示分类，p(ci|x,y)即表示在特征为x,y的情况下分入类别ci的概率，因此，结合条件概率和贝叶斯定理，有：

```
如果p(c1|x,y)>p(c2|x,y)，那么分类应当属于c1；
如果p(c1|x,y)<p(c2|x,y)，那么分类应当属于c2
```

`贝叶斯定理最大的好处是可以用已知的三个概率去计算未知的概率，而如果仅仅是为了比较p(ci|x,y)和p(cj|x,y)的大小，只需要已知两个概率即可，分母相同，比较p(x,y|ci)p(ci)和p(x,y|cj)p(cj)即可。`

(3) 特征条件独立假设原则

见QA4.

#### 三、朴素贝叶斯分类的流程

```
(1). 数据准备：收集数据，并将数据预处理为数值型或者布尔型，如对文本分类，需要将文本解析为词向量
(2). 训练数据：根据训练样本集计算词项出现的概率，训练数据后得到各类下词汇出现概率的向量
(3). 测试数据：用测试样本集去测试分类的准确性
```

#### 四、优缺点

```
1. 监督学习，需要确定分类的目标
2. 对缺失数据不敏感，在数据较少的情况下依然可以使用该方法
3. 可以处理多个类别 的分类问题
4. 适用于标称型数据
5. 对输入数据的形势比较敏感
6. 由于用先验数据去预测分类，因此存在误差
```

#### 五、平滑处理

有时候样本量较少的时候，无论该样本的其他属性是什么，计算出来的分类结果始终是一种分类，这样的情况不合理。为了避免上述弊端的出现，我们在计算概率值时要进行平滑处理，常用 [拉普拉斯平滑处理](https://www.cnblogs.com/bqtang/p/3693827.html)

$P(c) = \frac{|D_c+1|}{|D|+N}$

$P(x_i|c) = \frac{|D_{cx_i}+1|}{|D_c|+N_i}$

拉普拉斯修正避免了因训练集样本不充分而导致的概率值为零的问题。

- - -

参考：

0. [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)
1. [机器学习之朴素贝叶斯(NB)分类算法与Python实现](https://blog.csdn.net/moxigandashu/article/details/71480251)
2. [EM(最大期望)算法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95)

- - -

QA：

1. 基于概率论的方法和基于统计学的方法：

基于概率论的方法是通过概率来衡量事件发生的可能性。概率论和统计学恰好是两个相反的概念，统计学是抽取部分样本进行统计来估算总体的情况，而概率论是通过总体情况来估计单个事件或者部分事情的发生情况。

2. 朴素贝叶斯，朴素在什么地方：

之所以叫朴素贝叶斯，因为它简单、易于操作，基于特征独立性假设，假设各个特征不会相互影响，这样就大大减小了计算概率的难度。

3. 概率论中的基本概念

(1)事件交和并：

A和B两个事件的交，指的是事件A和B同时出现，记为A∩B;
A和B两个事件的并，指的是事件A和事件B至少出现一次的情况，记为A∪B。

(2)条件概率(conditional probability)：

某个事件发生时另外一个事件发生的概率，如事件A发生条件下事件B发生的概率：

$P(B|A)=\frac{P(A \cap B)}{P(A)} <=> P(B|A)=\frac{P(A|B)}{P(A)}P(B)$

$P(A \cap B)=P(A)P(B|A)  <=>  P(A \cap B)=P(B)P(A|B)$

- P(B)称为"先验概率"（Prior probability），即在A事件发生之前，我们对B事件概率的一个判断。
- P(B|A)称为"后验概率"（Posterior probability），即在A事件发生之后，我们对B事件概率的重新评估。
- P(A|B)/P(A)称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

所以，条件概率可以理解成下面的式子：

```
后验概率　＝　先验概率 ｘ 调整因子
```

(3)独立事件交的概率：

两个相互独立的事件，其交的概率为： $P(A \cap B)=P(A)P(B)$

4. 贝叶斯定理（Bayes’s Rule）：

如果有k个互斥且有穷个事件:B1,B2···，Bk ，并且，P(B1)+P(B2)+···+P(Bk)=1和一个可以观测到的事件A，那么有：

$P(B_i | A)=\frac{P(B_i \cap A)}{P(A)}=\frac{P(B_i)P(A | B_i)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+···+P(B_k)P(A|B_k)}$

- p(A) : 事件A发生的概率；
- p(B∩A) : 事件B 和事件A 同时发生的概率；
- p(B|B) : 表示事件B在事件A发生的条件下发生的概率。

- - -
THE END.
