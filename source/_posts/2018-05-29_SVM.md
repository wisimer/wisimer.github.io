---
title: 支持向量机 SVM
date: 2018-05-29
categories: ML
tags: [SVM]
---

支持向量机(support vector machine)，一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。

SVM 原理分为软间隔最大化、拉格朗日对偶、最优化问题求解、核函数、序列最小优化 SMO 等部分。

<!--more-->

#### 一、间隔和软间隔最大化

##### 1. 分隔超平面

![svm_intuition.png](/src/imgs/1805/0529_svm_intuition.png)

对于二分类问题来说，中间这条实线（或三维空间里的平面），就叫做分隔超平面，表达式为 $\vec w \cdot \vec x + b=0$。

##### 2. 函数间隔

SVM是通过超平面将样本分为两类。
在超平面 $w\cdot x+b=0$ 确定的情况下，$|w\cdot x+b|$ 可以相对地表示点x距离超平面的远近。对于两类分类问题，如果 $w\cdot x+b>0$ ，则x的类别被判定为1；否则判定为-1。

所以如果 $y(w\cdot x+b)>0$ ，则认为x的分类结果是正确的，否则是错误的。且 $y(w\cdot x+b)$ 的值越大，分类结果的确信度越大。反之亦然。所以样本点 $(x_{i}, y_{i})$ 与超平面(w, b)之间的函数间隔定义为:

$$\gamma_i=y_i(wx_i+b) \: (1.2.1)$$

我们用 $\hat{\gamma}$ 表示函数间隔，$\hat{\gamma}=y(w \cdot x+b)$。

##### 3. 几何间隔(软间隔)

上面函数间隔的定义存在问题：即w和b同时缩小或放大M倍后，超平面并没有变化，但是函数间隔却变化了。所以，需要将w的大小固定。例如||w||=1，使得函数间隔固定，这时的间隔也就是几何间隔 。几何间隔的定义如下:

$$\gamma_{i} = y_{i} (\frac{w}{||w||}\cdot x_{i} + \frac{b}{||w||}) = y_i(\frac{w\cdot x_i+b}{||w||}) \: (1.3.1)$$


其中 xi 代表第 i 条数据，yi 代表第 i 条数据对应的目标变量的取值，取值有+1 和-1 两种。所以当第 i 条数据被正确分类时，y 取值和 w*x+b 取值的正负一致，几何间隔为正；当被错误分类时，y 取值和 w*x+b 取值的正负相反，几何间隔为负。如下图：

![0529_svm_yi](/src/imgs/1805/0529_svm_yi.webp)

我们用 $\tilde{\gamma}$ 表示几何间隔，$\tilde{\gamma}=y\frac{w \cdot x+b}{||w||}$。由于 $\hat{\gamma}=y(w \cdot x+b)$，所以 $\tilde{\gamma}=\frac{\hat{\gamma}}{||w||}$。

此外，函数间隔以及几何间隔其实并不是表示某种距离，真正的点到直线（或平面）的距离公式是：$\gamma = \frac{w \cdot x + b}{||w||} \: (1.3.2)$。

小结一下：

(1). 实际距离 ：$\gamma = \frac{w \cdot x + b}{||w||} \: (1.3.3)$

(2). 函数距离 ：$\hat{\gamma}=y(w \cdot x+b) \: (1.3.4)$

(3). 几何距离 ：$\tilde{\gamma}=y\frac{w \cdot x+b}{||w||}=\frac{\hat{\gamma}}{||w||} \: (1.3.5)$

##### 4. 软间隔最大化

SVM 的核心思路是最大化支持向量到分隔超平面的间隔。后面所有的推导都是以最大化此间隔为核心思想展开。

(1). 首先定义几何间隔中最小的为：$\tilde{\gamma}=min\{\tilde{\gamma_i}\} \: (1.4.1)$ 。

(2). 由此可以得到间隔最大化问题的目标函数 $max \{\tilde{\gamma_i}\}$，并遵循如下约束条件：$s.t.\tilde{\gamma_i} = \frac{y_i(w\cdot x_i+b)}{||w||}\geq \tilde{\gamma}$，（s.t.表示 subject to：满足...限制） 。

这里重点解释一下上面这句话的意思：

> (1).几何间隔就是点到给定超平面的距离；几何间隔中最小值 $min\{\tilde{\gamma_i}\}$ 表示所有样本点中几何间隔的最小值。

> (2).间隔最大化问题 $max \{\tilde{\gamma}\}$ 表示要找到一种情况使得最小几何间隔在所有情况中取最大值，记为 $M = arg\:max_{w,b}\{\frac{min_{n}[y_i(w^T \cdot x_i+b)]}{||w||}\} \: (1.4.2)$。

##### 5. 目标函数

为了方便计算，令函数距离的最小值 $min\{\hat{\gamma}\}=min\{y(w \cdot x+b) \}= 1$，则 $M=arg \: max_{w,b}{\{\frac{1}{||w||}\}}$。很显然，`软间隔最大化` 就是要求M这个最大值，也就是要求 $\frac{1}{||w||}$的最大值，也就是要求 ||w|| 的最小值。此时几何间隔的最大化情况就是：$\tilde{\gamma} = \frac{1}{||w||}$。

于是就得出了目标函数 : $max \: {\frac{1}{||w||}},s.t.y_i(w \cdot x_i+b) \geq 1,i=1,2,...,n \: (1.5.1)$。

等价于 : 

$$min \: \frac{1}{2} ||w||^2,s.t.y_i(w \cdot x_i+b) \geq 1,i=1,2,...,n \: (1.5.2)$$

这个公式描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是 `支持向量机的基本数学模型`。


可以看到这样的问题满足如下两个条件：

(1). 它所最优化的问题是一个二次式

(2). 它的限制条件是一个线性的一次式

有这样特性的最优化问题就叫二次规划（quadratic programing ，简称QP）

#### 二、拉格朗日对偶（Lagrange duality）

##### 1. 最优化问题的分类

通常我们需要求解的最优化问题有如下几类：

(1). 无约束优化问题，可以写为: 

$$min\:f(x)\: (2.1.1)$$

对于第(1)类的优化问题，常常使用的方法就是 `Fermat定理(费马定理)`，即使用求取f(x)的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。

(2). 有等式约束的优化问题，可以写为: 

$$
min\:f(x);
s.t. h_i(x) = 0; 
i =1, ..., n 
\: (2.1.2)
$$

对于第(2)类的优化问题，常常使用的方法就是 `拉格朗日乘子法（Lagrange Multiplier)` ，即把等式约束h_i(x)用一个系数与f(x)写为一个式子，称为 `拉格朗日函数`，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。

(3). 有不等式约束的优化问题，可以写为：

$$
min\:f(x);
s.t. h_i(x) = 0; 
g_j(x) \leq 0; 
i =1, ..., n,j =1, ..., m
\: (2.1.3)
$$

对于第(3)类的优化问题，常常使用的方法就是 `KKT条件`。同样地，我们把所有的等式、不等式约束与f(x)写为一个式子，也叫 `拉格朗日函数`，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的必要条件，这个条件称为KKT条件。

##### 2. KKT条件

KKT条件的全称是Karush-Kuhn-Tucker条件，KKT条件是说最优值条件必须满足以下条件：

- 条件一：经过拉格朗日函数处理之后的新目标函数L(w,b,a)对x求导为零;
- 条件二：$h_i(x)=0$;
- 条件三：$a*g_j(x)=0$;

求取这三个等式之后就能得到候选最优值。其中第三个式子非常有趣，因为g(x)<=0，如果要满足这个等式，必须a=0或者g(x)=0。


##### 3. 拉格朗日对偶

由于这个问题的特殊结构，还可以通过拉格朗日对偶性变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是 `线性可分` 条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入 `核函数`，进而推广到 `非线性分类` 问题。

1). 首先回顾一下我们要处理的问题是 `有不等式约束的的优化问题`,这类问题的一般描述是：

$$
min\:f(x);
s.t. h_i(x) = 0;
g_j(x) \leq 0;
i =1, ..., n,j =1, ..., m
 \: (2.3.1)
$$

我们根据原来的目标函数来构造一个新的目标函数：

$$\theta_p(x)=max_{\alpha,\beta;\beta_j\geq0} \:L(x,\alpha,\beta)  \: (2.3.2)$$
 
2). 其中 $L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta})$ 为广义拉格朗日函数，定义为：

$$L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta})=f(\boldsymbol{x})+\sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x})  \: (2.3.3)$$

这里，$\boldsymbol{\alpha}=[\alpha_1,\alpha_2,\ldots, \alpha_m]^T,~\boldsymbol{\beta}=[\beta_1,\beta_2,\ldots, \beta_n]^T$，是我们在构造新目标函数时加入的系数变量，也是我们要求的参数。另外不要忘了我们要求的是 $\theta_p(x)$ 的最小值。

3). 根据上面(2.3.2)和(2.3.3)两个公式我们得到：

$$\theta_p(x)=max_{\alpha,\beta;\beta_j\geq0} \:\{f(\boldsymbol{x})+\sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x})\}  \: (2.3.4)$$

其中 $h_i(x) = 0;g_j(x) \leq 0;i =1, ..., n,j =1, ..., m$

首先如果x不满足这两条限制，也就是在 `可行解区域外`,我们观察一下公式(2.3.4)，一定可以找到某一种情况下的参数 $(\alpha,\beta)$ 使得 $\theta_p(x)$ 取向于 $\infty$，这种情况下肯定不是我们要求的$\theta_p(x)$ 的最小值。

再看一下x同时满足这两条显示，也就是在 `可行解区域内`,由于 $h_i(x) = 0;g_j(x) <= 0$，并且 $\beta_j \geq0$，那么很显然 $max_{\alpha,\beta;\beta_j\geq0} \:\{\sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x})\}=0$，于是可以得到 $\theta_p(x)$ 的取值分布情况：

$$\theta_P(x)=\left\{ \begin{array}{ll} f({x}) & {x}\in\Phi\\ \infty & \textrm{otherwise} \end{array} \right.$$

就这样原来有约束条件的 (2.3.1)问题求解现在就变成了没有约束条件求 $min \{\theta_p(x)\}$ 。

同时，我们设 $\theta_d(x)=min\{L(w,\alpha,\beta)\} \:(2.3.5)$，同时将问题转化为先求 $L(w,\alpha,\beta)$ 的最小值，再求它的最大值。与公式(2.3.2) $\theta_p(x)=max\{L(w,\alpha,\beta)\}$比较一下：

$$min\{\theta_p(x)\}=min\{max\{L(w,\alpha,\beta)\}\} \: (2.3.6)$$

$$max\{\theta_d(x)\}=max\{min\{L(w,\alpha,\beta)\}\} \: (2.3.7)$$

2.3.7 这个问题就是原问题 (2.3.6) 的对偶问题，相对于原问题只是更换了min和max的顺序，而一般更换顺序的结果是Max Min(X) <= Min Max(X)。然而在这里两者相等。由此我们可以设如下：

$$d^*=max\{min\{L(w,\alpha,\beta)\}\} \leq min\{max\{L(w,\alpha,\beta)\}\}=p^*$$

所以在一定的条件下我们可以得到：$d^{*}=p^{*}$，值得注意的是这里的问题要满足上面所说的 `KKT条件`。

##### 4. 对偶问题求解

1). 根据上面的推导我们知道了对偶问题的原理，回到一开始的问题，看一下公式 (1.5.2)，再将其转换为它的对偶问题可得：

$$d^{*}=max\{min\{L(w,\alpha,b)\}\}$$
$$L(x,\alpha,\beta)=\frac{1}{2}||w||^2-\sum_{i=1}^m\alpha_i [y_i(w^T \cdot x+b)-1]$$

2). 首先固定α，也就是将α视为常数，要让L(w,b,α)关于w和b最小化，我们分别对w和b偏导数，令其等于0，即：

$$\frac{\partial{L}}{\partial{w}}=0 \:=>\: w=\sum_{i=1}^{n}\alpha_i y_i x_i$$
$$\frac{\partial{L}}{\partial{b}}=0 \:=>\: \sum_{i=1}^{n}\alpha_i y_i=0$$

3). 将上述结果带回L(w,b,α)得到：

$$
\begin{array}{ll} L(w,\alpha,b) 
& =\frac{1}{2}||w||^2-\sum_{i=1}^m\alpha_i [y_i(w^T \cdot x+b)-1]\\ 
& =\frac{1}{2}w^T \cdot w-w^T\sum\alpha_iy_ix_i-b\sum\alpha_iy_i+\sum\alpha_i\\
& =\frac{1}{2}w^T \sum\alpha_iy_ix_i-w^T\sum\alpha_iy_ix_i-b\cdot0+\sum\alpha_i\\
& =\sum\alpha_i-\frac{1}{2}(\sum\alpha_iy_ix_i)^T\sum\alpha_iy_ix_i\\
& =\sum\alpha_i-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_i \alpha_j y_i y_j x_i^T x_i\\
&  \end{array} 
$$

从上面的最后一个式子，我们可以看出，此时的L(w,α,b)函数只含有一个变量，即 $\alpha_i$。

4). 现在内侧的最小值求解完成，我们求解外侧的最大值，从上面的式子得到:

$$max\{\sum\alpha_i-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_i \alpha_j y_i y_j x_i^T x_i\}$$

$$s.t. \alpha_i\geq0,i=1,2,...,n$$

$$\sum_{i=1}^{n}\alpha_i y_i=0$$

现在我们的优化问题变成了如上的形式。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。我们通过这个优化算法能得到α，再根据α，我们就可以求解出w和b，进而求得我们最初的目的：找到超平面，即"决策平面"。

#### 三、SMO(Sequential Minimal Optimizaion)算法


#### 四、线性不可分

#### 五、核函数
- - -

参考：

1. [Python3《机器学习实战》学习笔记（八）：支持向量机原理篇之手撕线性SVM](https://zhuanlan.zhihu.com/p/29604517)

2. [支持向量机的原理和实现](https://mp.weixin.qq.com/s/zHCgOHsBYCkPouBFTmWgpw)

3. [机器学习与数据挖掘-支持向量机(SVM)](https://wizardforcel.gitbooks.io/dm-algo-top10/content/svm-1.html)

4. [支持向量机(SVM)是什么意思？](https://www.zhihu.com/question/21094489)

5. [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)

6. [Support Vector Machines](http://cs229.stanford.edu/notes/cs229-notes3.pdf)

7. [零基础学SVM—Support Vector Machine(一)](https://zhuanlan.zhihu.com/p/24638007)
- - -
THE END.
